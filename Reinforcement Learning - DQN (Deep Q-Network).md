### üß† What is **DQN (Deep Q-Network)?**

**DQN** is a powerful reinforcement learning algorithm that combines:
- **Q-Learning** (model-free, off-policy RL)
- **Deep Neural Networks** (to approximate Q-values)

> Instead of using a Q-table (which doesn‚Äôt scale to large or continuous state spaces), DQN uses a **neural network to approximate Q(s, a)**.

It was first introduced by **DeepMind** and made famous by its success on **Atari games**.

---

### üìê Core Idea of DQN

- Use a **neural network** \( Q(s, a; \theta) \) to approximate the Q-values.
- Learn the weights \( \theta \) of the network by minimizing a **loss function** based on the **Q-learning update rule**.

---

### üßÆ DQN Loss Function

$$\mathcal{L}(\theta) = \left( r + \gamma \cdot \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2$$

Where:
- \( \theta \): current network weights
- \( \theta^- \): **target network weights** (a periodically updated copy of the main network)
- \( Q(s, a; \theta) \): predicted Q-value from the network
- $$\( r + \gamma \cdot \max Q(s', a') \)$$: target Q-value

---

### ‚öôÔ∏è Training Algorithm for DQN

```text
1. Initialize:
    - Replay buffer D (for experience replay)
    - Q-network with random weights Œ∏
    - Target network Q_target with weights Œ∏‚Åª = Œ∏

2. For each episode:
    a. Initialize environment, get initial state s
    b. Repeat until episode ends:
        i.   Choose action a using Œµ-greedy policy from Q(s, a; Œ∏)
        ii.  Take action a, observe reward r and next state s'
        iii. Store (s, a, r, s') in replay buffer D
        iv.  Sample a mini-batch of transitions from D
        v.   For each transition (s, a, r, s'):
                - Compute target: 
                  y = r + Œ≥ * max_a' Q_target(s', a'; Œ∏‚Åª)
                - Compute loss: 
                  L = (y - Q(s, a; Œ∏))¬≤
        vi.  Perform gradient descent on L to update Œ∏
        vii. Periodically update target network: Œ∏‚Åª ‚Üê Œ∏
```

### üß† Key Techniques in DQN

| Technique              | Purpose |
|------------------------|--------|
| **Experience Replay**  | Store past experiences and sample randomly to break correlation and stabilize learning. |
| **Target Network**     | Use a separate, slowly-updated Q-network to compute target values for more stable learning. |
| **Œµ-Greedy Exploration** | Balance exploration and exploitation. |

---

### üîç Inference with DQN

After training, inference is simple:

```text
Given a state s:
    Choose action a = argmax_a Q(s, a; Œ∏)
```

So the agent just passes the current state into the trained Q-network and selects the action with the highest predicted Q-value.

---

### üìä Summary Table

| Component               | DQN Model |
|------------------------|------------------------------|
| Type                   | Model-free, off-policy RL |
| Q-function             | Approximated with neural network |
| Training goal          | Minimize difference between predicted and target Q-values |
| Replay Buffer          | Helps reduce data correlation |
| Target Network         | Stabilizes learning |
| Inference              | `a = argmax_a Q(s, a; Œ∏)` |

---

### üÜö DQN vs Q-Learning Recap

| Aspect            | Q-Learning         | DQN                         |
|-------------------|-------------------|-----------------------------|
| State space       | Small/discrete     | Large/continuous            |
| Q-function        | Table              | Neural Network              |
| Stability tricks  | None               | Experience replay + target net |
