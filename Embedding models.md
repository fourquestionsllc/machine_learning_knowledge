Here's a concise yet comprehensive comparison of **widely used embedding models** from major providers (OpenAI, AWS, Google, Azure, and others like Cohere, Mistral, and Hugging Face). I‚Äôll cover:

1. **Popular embedding models**
2. **How they work**
3. **How to use them**
4. **Embedding dimensions**
5. **Normalization**
6. **Similarity metrics and ranges**

---

## üî• 1. Popular Embedding Models (2024‚Äì2025)

| Provider        | Model Name                                      | Dim      | Normalized? | Notes                      |
| --------------- | ----------------------------------------------- | -------- | ----------- | -------------------------- |
| **OpenAI**      | `text-embedding-3-small`                        | 1536     | ‚úÖ Yes       | Fast, cheap, new standard  |
|                 | `text-embedding-3-large`                        | 3072     | ‚úÖ Yes       | Higher quality             |
| **AWS**         | Amazon Titan Text Embeddings G1                 | 1536     | ‚ùå No        | Powerful multilingual      |
| **Google**      | Vertex AI `textembedding-gecko`                 | 768      | ‚úÖ Yes       | Good general-purpose       |
| **Azure**       | Mirrors OpenAI models                           | Same     | Same        | Uses OpenAI under hood     |
| **Cohere**      | `embed-english-v3.0`, `embed-multilingual-v3.0` | 1024     | ‚úÖ Yes       | High quality, multilingual |
| **HuggingFace** | `BAAI/bge-large-en`, `all-MiniLM-L6-v2`, etc.   | 384‚Äì1024 | ‚ùå (varies)  | Open source, flexible      |

---

## ‚öôÔ∏è 2. How Embeddings Work (Briefly)

Embeddings are fixed-size vector representations of text (words, sentences, or documents), where **semantic similarity** corresponds to **geometric closeness** in vector space. They're generated by transformer models trained with contrastive loss or other objectives like MLM + alignment.

---

## üõ†Ô∏è 3. How to Use Them

### OpenAI Example:

```python
import openai

response = openai.embeddings.create(
    model="text-embedding-3-small",
    input="The quick brown fox jumps over the lazy dog"
)
embedding = response['data'][0]['embedding']
```

### Hugging Face Example:

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('BAAI/bge-large-en')
embedding = model.encode("The quick brown fox...", normalize_embeddings=True)
```

### Google Vertex Example:

```python
from vertexai.language_models import TextEmbeddingModel

model = TextEmbeddingModel.from_pretrained("textembedding-gecko@001")
embedding = model.get_embeddings(["text here"]).embeddings[0].values
```

### AWS Bedrock (Pseudocode):

```python
client = boto3.client('bedrock-runtime')
response = client.invoke_model(body={"inputText": "..."}, modelId="amazon.titan-embed-text-v1")
embedding = response['embedding']
```

---

## üìè 4. Dimensions

| Model                    | Dimensionality |
| ------------------------ | -------------- |
| `text-embedding-3-small` | 1536           |
| `text-embedding-3-large` | 3072           |
| Amazon Titan             | 1536           |
| Cohere v3                | 1024           |
| Gecko                    | 768            |
| BAAI BGE / MiniLM        | 384‚Äì1024       |

---

## ‚öñÔ∏è 5. Are They Normalized?

| Model                        | Normalized Output   |
| ---------------------------- | ------------------- |
| OpenAI (`text-embedding-3*`) | ‚úÖ Yes (L2 norm = 1) |
| Cohere v3                    | ‚úÖ Yes               |
| Google Gecko                 | ‚úÖ Yes               |
| AWS Titan                    | ‚ùå No                |
| Hugging Face                 | ‚ùå (often optional)  |

üëâ If not normalized, normalize manually using L2:

```python
from numpy import linalg as LA
embedding = embedding / LA.norm(embedding)
```

---

## üîç 6. Similarity Metric

**Use Cosine Similarity** for most embedding models, especially if vectors are normalized:

```python
from numpy import dot
from numpy.linalg import norm

def cosine_similarity(a, b):
    return dot(a, b) / (norm(a) * norm(b))
```

### üîÑ Normalized vectors:

* Cosine similarity becomes a **dot product**
* Range: **\[-1, 1]**
* Most models output **positive scores** (0 to 1) if inputs are similar types

### ‚ú≥Ô∏è Alternative: Euclidean Distance

* Less common for text embeddings
* Use if the model doesn't normalize and you want absolute geometric distance

---

## üß† Tips for Usage in RAG / Search / Clustering

* Use **cosine similarity** over **normalized embeddings** for:

  * Semantic search
  * RAG
  * Duplicate detection
* Consider **dimensionality reduction** (e.g., PCA) for visualization
* Use **FAISS** or **PGVector** for scalable similarity search

---

## üîö TL;DR

| Provider     | Best Model               | Dim  | Norm? | Compare With         | Score Range |
| ------------ | ------------------------ | ---- | ----- | -------------------- | ----------- |
| OpenAI       | `text-embedding-3-small` | 1536 | ‚úÖ     | Cosine               | \~0‚Äì1       |
| AWS          | Titan Embeddings         | 1536 | ‚ùå     | Cosine or Euclidean  | \~-1‚Äì1      |
| Google       | Gecko                    | 768  | ‚úÖ     | Cosine               | 0‚Äì1         |
| Cohere       | `embed-english-v3.0`     | 1024 | ‚úÖ     | Cosine               | 0‚Äì1         |
| Hugging Face | `BAAI/bge-*`             | 768+ | ‚ùå     | Cosine (manual norm) | \~-1‚Äì1      |

